services:
  qwen2-vl:
    build:
      context: .
      dockerfile: Dockerfile
    image: qwen2-vl:latest
    container_name: qwen2-vl

    # GPU 설정
    deploy:
      resources:
        limits:
          memory: 22G
        reservations:
          memory: 22G
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

    ports:
      - "7860:7860"   # Gradio UI

    # 볼륨 마운트
    volumes:
      # 모델 캐시 디렉토리 (HuggingFace cache)
      - ./models:/workspace/models
      # 입력 비디오 디렉토리
      - ./videos:/workspace/videos
      # 출력 결과 저장 디렉토리
      - ./outputs:/workspace/outputs
      # 소스코드 (app.py 등)
      - ./workspace:/workspace/app

    environment:
      # GPU 선택 (필요 시 GPU ID 지정)
      - CUDA_VISIBLE_DEVICES=0
      - HF_HOME=/workspace/models
      - TRANSFORMERS_CACHE=/workspace/models
      - HF_HUB_CACHE=/workspace/models
      - PYTHONUNBUFFERED=1
      - TZ=Asia/Seoul

    stdin_open: true
    tty: true
    restart: unless-stopped

    # 공유 메모리 확보 (영상 스트림 안정화)
    shm_size: "16gb"

    working_dir: /workspace/app

    # 시작 명령
    command: [
      "python", 
      "gradio_qwen_vl.py",  # ← 너의 실행 스크립트명으로 수정
      "--server-port", "7860",
      "--checkpoint-path", "Qwen/Qwen2-VL-7B-Instruct"
    ]
